{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/Users/harikrishnanagarajan/Downloads/obama-tweets.csv'\n",
    "df = pd.read_csv(dataset_path, usecols= ['Anootated tweet', 'Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anootated tweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question: If &lt;e&gt;Romney&lt;/e&gt; and &lt;e&gt;Obama&lt;/e&gt; ha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @davewiner Slate: Blame &lt;e&gt;Obama&lt;/e&gt; for fo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Hollivan @hereistheanswer  Youre missing the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt; made all of his money himse...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I was raised as a Democrat  left the party yea...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The &lt;e&gt;Obama camp&lt;/e&gt; can't afford to lower ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tonight's debate has that \"Game 7\" feel! This ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;e&gt;Obama&lt;/e&gt; pot &lt;a&gt;policy&lt;/a&gt; disappointing -...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Anootated tweet Class\n",
       "0  Kirkpatrick, who wore a baseball cap embroider...     0\n",
       "1  Question: If <e>Romney</e> and <e>Obama</e> ha...     2\n",
       "2  #<e>obama</e> debates that Cracker Ass Cracker...     1\n",
       "3  RT @davewiner Slate: Blame <e>Obama</e> for fo...     2\n",
       "4  @Hollivan @hereistheanswer  Youre missing the ...     0\n",
       "5  <e>Mitt Romney</e> made all of his money himse...     2\n",
       "6  I was raised as a Democrat  left the party yea...    -1\n",
       "7  The <e>Obama camp</e> can't afford to lower ex...     0\n",
       "8  Tonight's debate has that \"Game 7\" feel! This ...     2\n",
       "9  <e>Obama</e> pot <a>policy</a> disappointing -...    -1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    # Remove link,user and special characters\n",
    "    tweet = str(tweet).replace('<e>','')\n",
    "    tweet = str(tweet).replace('</e>', '')\n",
    "    tweet = re.sub(TEXT_CLEANING_RE, ' ', str(tweet).lower()).strip()\n",
    "    \n",
    "    tokens = []\n",
    "    for token in tweet.split():\n",
    "        if token not in stop_words:\n",
    "            tokens.append(stemmer.stem(token))\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Anootated tweet'] = df['Anootated tweet'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(data):\n",
    "    data = np.asarray(data)\n",
    "    temp = np.zeros((len(data),3))\n",
    "    for i in range(len(temp)):\n",
    "        if data[i] == '1':\n",
    "            temp[i][0] = 1\n",
    "        elif data[i] == '0':\n",
    "            temp[i][1] = 1\n",
    "        elif data[i] == '2':\n",
    "            temp[i][2] = 1\n",
    "\n",
    "    return temp        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    temp = []\n",
    "    for i in x:\n",
    "        m = np.argmax(i)\n",
    "        if m == 0:\n",
    "            temp.append('1')\n",
    "        elif m == 1:\n",
    "            temp.append('0')\n",
    "        else:\n",
    "            temp.append('2')\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset= ['Class', 'Anootated tweet'])\n",
    "df = df[df.Class != '2']\n",
    "df = df[df.Class != 'irrelevant']\n",
    "df = df[df.Class != 'irrevelant']\n",
    "df = df.replace('-1', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anootated tweet</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick wore basebal cap embroid barack ob...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>obama debat cracker ass cracker tonight tune t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hereistheansw your miss point im afraid unders...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rais democrat left parti year ago 1980 lifetim...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>obama camp afford lower expect tonight debat p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>obama pot polici disappoint say least 420 lega...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hollywood back rt redalert gene simmon yank ob...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>obama expedi speak fair order slender biscuit ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dream smoke obama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>washington time presid popular bubbl burst bar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Anootated tweet Class\n",
       "0  kirkpatrick wore basebal cap embroid barack ob...     0\n",
       "1  obama debat cracker ass cracker tonight tune t...     1\n",
       "2  hereistheansw your miss point im afraid unders...     0\n",
       "3  rais democrat left parti year ago 1980 lifetim...     2\n",
       "4  obama camp afford lower expect tonight debat p...     0\n",
       "5  obama pot polici disappoint say least 420 lega...     2\n",
       "6  hollywood back rt redalert gene simmon yank ob...     2\n",
       "7  obama expedi speak fair order slender biscuit ...     0\n",
       "8                                  dream smoke obama     0\n",
       "9  washington time presid popular bubbl burst bar...     2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index()\n",
    "df = df.drop(['index'], axis= 1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_size: 4781\n",
      "\n",
      "Test_size: 844\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size= 0.15, random_state= 5) \n",
    "print('Train_size:', len(df_train))\n",
    "print('\\nTest_size:', len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in the vocab: 7161\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train['Anootated tweet'])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Total words in the vocab:', vocab_size)\n",
    "\n",
    "SEQ_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(tokenizer.texts_to_sequences(df_train['Anootated tweet']), maxlen= SEQ_LENGTH, padding='post', truncating = 'pre')\n",
    "\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test['Anootated tweet']), maxlen= SEQ_LENGTH, padding='post', truncating = 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (4781, 100)\n",
      "Shape of y_train: (4781, 3)\n",
      "Shape of x_test: (844, 100)\n",
      "Shape of y_test: (844, 3)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(oneHotEncode(df_train.Class))\n",
    "\n",
    "y_test = np.array(oneHotEncode(df_test.Class))\n",
    "\n",
    "print('Shape of x_train:', x_train.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "print('Shape of x_test:', x_test.shape)\n",
    "print('Shape of y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "\n",
    "with open('/Users/harikrishnanagarajan/Downloads/glove/glove.twitter.27B.200d.txt', 'r') as f:\n",
    "    \n",
    "    for line in f:\n",
    "        \n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:], \"float32\")\n",
    "        embedding_dict[word] = vectors\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 200))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    \n",
    "    if i < vocab_size:\n",
    "        \n",
    "        emb_vec = embedding_dict.get(word)\n",
    "        if emb_vec is not None:\n",
    "            \n",
    "            embedding_matrix[i] = emb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def f1_value(y_true, y_pred): #taken from old keras source code\n",
    "    \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    \n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    \n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILDING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM, Bidirectional, SimpleRNN, GRU\n",
    "from keras import utils\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 75, 200)           1432200   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 603       \n",
      "=================================================================\n",
      "Total params: 1,673,603\n",
      "Trainable params: 241,403\n",
      "Non-trainable params: 1,432,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, peoch, logs= {}):\n",
    "        if logs.get('val_f1_value') > 0.58:\n",
    "            print(\"Ending Training\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "callback = myCallback()\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 200, weights= [embedding_matrix], input_length= SEQ_LENGTH, trainable= False)\n",
    "\n",
    "\n",
    "model_LSTM = Sequential()\n",
    "\n",
    "model_LSTM.add(embedding_layer)\n",
    "model_LSTM.add(Bidirectional(LSTM(100, activation= 'tanh')))\n",
    "model_LSTM.add(Dropout(0.2))\n",
    "model_LSTM.add(Dense(3, activation= 'softmax'))\n",
    "\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4302 samples, validate on 479 samples\n",
      "Epoch 1/20\n",
      "4302/4302 [==============================] - 23s 5ms/step - loss: 0.9725 - f1_value: 0.4182 - val_loss: 0.9381 - val_f1_value: 0.4588\n",
      "Epoch 2/20\n",
      "4302/4302 [==============================] - 22s 5ms/step - loss: 0.8261 - f1_value: 0.5857 - val_loss: 0.9341 - val_f1_value: 0.5175\n",
      "Epoch 3/20\n",
      "4302/4302 [==============================] - 22s 5ms/step - loss: 0.7299 - f1_value: 0.6629 - val_loss: 0.9886 - val_f1_value: 0.5530\n",
      "Epoch 4/20\n",
      "4302/4302 [==============================] - 22s 5ms/step - loss: 0.5520 - f1_value: 0.7728 - val_loss: 0.9975 - val_f1_value: 0.5674\n",
      "Epoch 5/20\n",
      "4302/4302 [==============================] - 22s 5ms/step - loss: 0.4017 - f1_value: 0.8406 - val_loss: 1.1881 - val_f1_value: 0.5457\n",
      "Epoch 6/20\n",
      "4302/4302 [==============================] - 23s 5ms/step - loss: 0.2566 - f1_value: 0.9102 - val_loss: 1.4033 - val_f1_value: 0.5707\n",
      "Epoch 7/20\n",
      "4302/4302 [==============================] - 21s 5ms/step - loss: 0.1761 - f1_value: 0.9411 - val_loss: 1.5533 - val_f1_value: 0.5855\n",
      "Ending Training\n"
     ]
    }
   ],
   "source": [
    "opt = adam(learning_rate= 0.007)\n",
    "model_LSTM.compile(loss= 'categorical_crossentropy', optimizer= opt, metrics= [f1_value])\n",
    "\n",
    "history = model_LSTM.fit(x_train, y_train, epochs= 20, validation_split= 0.1, batch_size= 64 ,verbose= 1, callbacks= [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM.save('BEST_LSTM_MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 100, 200)          1432200   \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 160)               44960     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 483       \n",
      "=================================================================\n",
      "Total params: 1,477,643\n",
      "Trainable params: 45,443\n",
      "Non-trainable params: 1,432,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, peoch, logs= {}):\n",
    "        if logs.get('val_f1_value') > 0.55:\n",
    "            print(\"Ending Training\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "callback = myCallback()\n",
    "            \n",
    "embedding_layer = Embedding(vocab_size, 200, weights= [embedding_matrix], input_length= SEQ_LENGTH, trainable= False)\n",
    "\n",
    "\n",
    "model_Vanilla = Sequential()\n",
    "\n",
    "model_Vanilla.add(embedding_layer)\n",
    "model_Vanilla.add(Bidirectional(SimpleRNN(80, activation= 'tanh')))\n",
    "model_Vanilla.add(Dropout(0.2))\n",
    "model_Vanilla.add(Dense(3, activation= 'softmax'))\n",
    "\n",
    "model_Vanilla.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4302 samples, validate on 479 samples\n",
      "Epoch 1/20\n",
      "4302/4302 [==============================] - 7s 2ms/step - loss: 1.1306 - f1_value: 0.3599 - val_loss: 1.0544 - val_f1_value: 0.4393\n",
      "Epoch 2/20\n",
      "4302/4302 [==============================] - 6s 1ms/step - loss: 0.9848 - f1_value: 0.4837 - val_loss: 1.0778 - val_f1_value: 0.3983\n",
      "Epoch 3/20\n",
      "4302/4302 [==============================] - 6s 1ms/step - loss: 0.9622 - f1_value: 0.4772 - val_loss: 1.0507 - val_f1_value: 0.2506\n",
      "Epoch 4/20\n",
      "4302/4302 [==============================] - 6s 1ms/step - loss: 1.0927 - f1_value: 0.3613 - val_loss: 1.0812 - val_f1_value: 0.3856\n",
      "Epoch 5/20\n",
      "4302/4302 [==============================] - 5s 1ms/step - loss: 0.9902 - f1_value: 0.4524 - val_loss: 1.0374 - val_f1_value: 0.4049\n",
      "Epoch 6/20\n",
      "4302/4302 [==============================] - 5s 1ms/step - loss: 0.9206 - f1_value: 0.5135 - val_loss: 1.0099 - val_f1_value: 0.4589\n",
      "Epoch 7/20\n",
      "4302/4302 [==============================] - 5s 1ms/step - loss: 0.8750 - f1_value: 0.5758 - val_loss: 1.0686 - val_f1_value: 0.4736\n",
      "Epoch 8/20\n",
      "4302/4302 [==============================] - 5s 1ms/step - loss: 0.8275 - f1_value: 0.6088 - val_loss: 1.0474 - val_f1_value: 0.5066\n",
      "Epoch 9/20\n",
      "4302/4302 [==============================] - 5s 1ms/step - loss: 0.7933 - f1_value: 0.6401 - val_loss: 1.1319 - val_f1_value: 0.4787\n",
      "Epoch 10/20\n",
      "4302/4302 [==============================] - 5s 1ms/step - loss: 0.8074 - f1_value: 0.6244 - val_loss: 1.0600 - val_f1_value: 0.4997\n",
      "Epoch 11/20\n",
      "4302/4302 [==============================] - 6s 1ms/step - loss: 0.7334 - f1_value: 0.6775 - val_loss: 1.0817 - val_f1_value: 0.5298\n",
      "Epoch 12/20\n",
      "4302/4302 [==============================] - 5s 1ms/step - loss: 0.7343 - f1_value: 0.6723 - val_loss: 1.1024 - val_f1_value: 0.5102\n",
      "Epoch 13/20\n",
      "4302/4302 [==============================] - 5s 1ms/step - loss: 0.6517 - f1_value: 0.7287 - val_loss: 1.1086 - val_f1_value: 0.5340\n",
      "Epoch 14/20\n",
      "4302/4302 [==============================] - 5s 1ms/step - loss: 0.6108 - f1_value: 0.7501 - val_loss: 1.2019 - val_f1_value: 0.5350\n",
      "Epoch 15/20\n",
      "4302/4302 [==============================] - 6s 1ms/step - loss: 0.5756 - f1_value: 0.7628 - val_loss: 1.2435 - val_f1_value: 0.5187\n",
      "Epoch 16/20\n",
      "4302/4302 [==============================] - 6s 1ms/step - loss: 0.5707 - f1_value: 0.7653 - val_loss: 1.2599 - val_f1_value: 0.5330\n",
      "Epoch 17/20\n",
      "4302/4302 [==============================] - 6s 1ms/step - loss: 0.4986 - f1_value: 0.8070 - val_loss: 1.3094 - val_f1_value: 0.5317\n",
      "Epoch 18/20\n",
      "4302/4302 [==============================] - 6s 1ms/step - loss: 0.4684 - f1_value: 0.8168 - val_loss: 1.3677 - val_f1_value: 0.5320\n",
      "Epoch 19/20\n",
      "4302/4302 [==============================] - 5s 1ms/step - loss: 0.4155 - f1_value: 0.8399 - val_loss: 1.3902 - val_f1_value: 0.5328\n",
      "Epoch 20/20\n",
      "4302/4302 [==============================] - 6s 1ms/step - loss: 0.3937 - f1_value: 0.8530 - val_loss: 1.3925 - val_f1_value: 0.5312\n"
     ]
    }
   ],
   "source": [
    "opt1 = adam(learning_rate= 0.007)\n",
    "model_Vanilla.compile(loss= 'categorical_crossentropy', optimizer= opt1, metrics= [f1_value])\n",
    "\n",
    "history1 = model_Vanilla.fit(x_train, y_train, epochs= 20, validation_split= 0.1, batch_size= 128, verbose= 1, callbacks= [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Vanilla.save('BEST_VANILLA_MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 100, 200)          1432200   \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 200)               180600    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 603       \n",
      "=================================================================\n",
      "Total params: 1,613,403\n",
      "Trainable params: 181,203\n",
      "Non-trainable params: 1,432,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, peoch, logs= {}):\n",
    "        if logs.get('val_f1_value') > 0.57:\n",
    "            print(\"Ending Training\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "callback = myCallback()\n",
    "embedding_layer = Embedding(vocab_size, 200, weights= [embedding_matrix], input_length= SEQ_LENGTH, trainable= False)\n",
    "\n",
    "\n",
    "model_GRU = Sequential()\n",
    "\n",
    "model_GRU.add(embedding_layer)\n",
    "model_GRU.add(Bidirectional(GRU(100, activation= 'tanh')))\n",
    "model_GRU.add(Dropout(0.2))\n",
    "model_GRU.add(Dense(3, activation= 'softmax'))\n",
    "\n",
    "model_GRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4302 samples, validate on 479 samples\n",
      "Epoch 1/20\n",
      "4302/4302 [==============================] - 34s 8ms/step - loss: 1.0102 - f1_value: 0.3389 - val_loss: 0.9747 - val_f1_value: 0.4441\n",
      "Epoch 2/20\n",
      "4302/4302 [==============================] - 28s 7ms/step - loss: 0.8507 - f1_value: 0.5652 - val_loss: 0.9387 - val_f1_value: 0.4707\n",
      "Epoch 3/20\n",
      "4302/4302 [==============================] - 30s 7ms/step - loss: 0.7664 - f1_value: 0.6380 - val_loss: 0.9840 - val_f1_value: 0.5385\n",
      "Epoch 4/20\n",
      "4302/4302 [==============================] - 26s 6ms/step - loss: 0.6562 - f1_value: 0.7147 - val_loss: 1.0161 - val_f1_value: 0.5428\n",
      "Epoch 5/20\n",
      "4302/4302 [==============================] - 29s 7ms/step - loss: 0.5116 - f1_value: 0.7896 - val_loss: 1.1029 - val_f1_value: 0.5677\n",
      "Epoch 6/20\n",
      "4302/4302 [==============================] - 27s 6ms/step - loss: 0.3441 - f1_value: 0.8714 - val_loss: 1.2294 - val_f1_value: 0.5849\n",
      "Ending Training\n"
     ]
    }
   ],
   "source": [
    "opt2 = adam(learning_rate= 0.005)\n",
    "model_GRU.compile(loss= 'categorical_crossentropy', optimizer= opt2, metrics= [f1_value])\n",
    "\n",
    "history2 = model_GRU.fit(x_train, y_train, epochs= 20, validation_split= 0.1, verbose= 1, batch_size=128, callbacks= [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU.save('BEST_GRU_MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "loaded_model = keras.models.load_model('BEST_GRU_MODEL', custom_objects= {'f1_value': f1_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = loaded_model.predict(x_test)\n",
    "f1_list= np.round(f1_score(pred(y_test), pred(y_pred), average = None),3)\n",
    "accuracy = accuracy_score(pred(y_test), pred(y_pred))\n",
    "f1_dict = {'f1_pos': f1_list[0], 'f1_neu': f1_list[1], 'f1_neg': f1_list[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_pos': 0.565, 'f1_neu': 0.529, 'f1_neg': 0.579}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5604265402843602"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
